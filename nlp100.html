
<!DOCTYPE html>


<html lang="ja" xmlns:fb="http://ogp.me/ns/fb#">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="言語処理100本ノックは，実践的な課題に取り組みながら，プログラミング，データ分析，研究のスキルを楽しく習得することを目指した問題集です">
    <meta name="author" content="Naoaki Okazaki, Inui-Okazaki Laboratory">
    <link rel="icon" href="favicon.ico">

    <title>言語処理100本ノック 2015</title>

    <!-- Bootstrap core CSS -->
    <link href="bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this page -->
    <style type="text/css">
      body {min-height: 2000px; padding-top: 70px;}
      .carousel-control.left, .carousel-control.right {
      background-image: none
      }
      .jumbotron h1 {font-size: xx-large;}
      footer {padding-top:40px;padding-bottom:40px;margin-top:100px;color:#767676;text-align:center;border-top:1px solid #e5e5e5}
      footer p {margin-bottom:0}
      .fb-like > span {
      vertical-align: top !important;
      }
    </style>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

    <script type="text/javascript"><!--
    var shiftWindow = function() { scrollBy(0, -70) };
    if (location.hash) shiftWindow();
    window.addEventListener("hashchange", shiftWindow);
    function load() { if (window.location.hash) shiftWindow(); }
    --></script>

    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>

    <!-- Google+ -->
    <script src="https://apis.google.com/js/platform.js" async defer>
      {lang: 'ja'}
    </script>

    <!-- Hatena -->
    <script type="text/javascript" src="https://b.st-hatena.com/js/bookmark_button.js" charset="utf-8" async="async"></script>
  </head>

  <body onload="load()">
    <div id="fb-root"></div>
    <script>(function(d, s, id) {
      var js, fjs = d.getElementsByTagName(s)[0];
      if (d.getElementById(id)) return;
      js = d.createElement(s); js.id = id;
      js.src = "//connect.facebook.net/ja_JP/sdk.js#xfbml=1&appId=200896499933736&version=v2.0";
      fjs.parentNode.insertBefore(js, fjs);
      }(document, 'script', 'facebook-jssdk'));</script>
    
    <!-- Fixed navbar -->
    <nav class="navbar navbar-default navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">言語処理100本ノック</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            
            <li class="dropdown">
              <a href="#ch1" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">第1章<span class="caret"></span></a>
              <ul class="dropdown-menu" role="menu">
                <li><a href="#ch1">第1章: 準備運動</a></li>
                
                <li><a href="#sec00">00. 文字列の逆順</a></li>
                
                <li><a href="#sec01">01. 「パタトクカシーー」</a></li>
                
                <li><a href="#sec02">02. 「パトカー」＋「タクシー」＝「パタトクカシーー」</a></li>
                
                <li><a href="#sec03">03. 円周率</a></li>
                
                <li><a href="#sec04">04. 元素記号</a></li>
                
                <li><a href="#sec05">05. n-gram</a></li>
                
                <li><a href="#sec06">06. 集合</a></li>
                
                <li><a href="#sec07">07. テンプレートによる文生成</a></li>
                
                <li><a href="#sec08">08. 暗号文</a></li>
                
                <li><a href="#sec09">09. Typoglycemia</a></li>
                
              </ul>
            </li>
            
            <li class="dropdown">
              <a href="#ch2" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">第2章<span class="caret"></span></a>
              <ul class="dropdown-menu" role="menu">
                <li><a href="#ch2">第2章: UNIXコマンドの基礎</a></li>
                
                <li><a href="#sec10">10. 行数のカウント</a></li>
                
                <li><a href="#sec11">11. タブをスペースに置換</a></li>
                
                <li><a href="#sec12">12. 1列目をcol1.txtに，2列目をcol2.txtに保存</a></li>
                
                <li><a href="#sec13">13. col1.txtとcol2.txtをマージ</a></li>
                
                <li><a href="#sec14">14. 先頭からN行を出力</a></li>
                
                <li><a href="#sec15">15. 末尾のN行を出力</a></li>
                
                <li><a href="#sec16">16. ファイルをN分割する</a></li>
                
                <li><a href="#sec17">17. １列目の文字列の異なり</a></li>
                
                <li><a href="#sec18">18. 各行を3コラム目の数値の降順にソート</a></li>
                
                <li><a href="#sec19">19. 各行の1コラム目の文字列の出現頻度を求め，出現頻度の高い順に並べる</a></li>
                
              </ul>
            </li>
            
            <li class="dropdown">
              <a href="#ch3" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">第3章<span class="caret"></span></a>
              <ul class="dropdown-menu" role="menu">
                <li><a href="#ch3">第3章: 正規表現</a></li>
                
                <li><a href="#sec20">20. JSONデータの読み込み</a></li>
                
                <li><a href="#sec21">21. カテゴリ名を含む行を抽出</a></li>
                
                <li><a href="#sec22">22. カテゴリ名の抽出</a></li>
                
                <li><a href="#sec23">23. セクション構造</a></li>
                
                <li><a href="#sec24">24. ファイル参照の抽出</a></li>
                
                <li><a href="#sec25">25. テンプレートの抽出</a></li>
                
                <li><a href="#sec26">26. 強調マークアップの除去</a></li>
                
                <li><a href="#sec27">27. 内部リンクの除去</a></li>
                
                <li><a href="#sec28">28. MediaWikiマークアップの除去</a></li>
                
                <li><a href="#sec29">29. 国旗画像のURLを取得する</a></li>
                
              </ul>
            </li>
            
            <li class="dropdown">
              <a href="#ch4" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">第4章<span class="caret"></span></a>
              <ul class="dropdown-menu" role="menu">
                <li><a href="#ch4">第4章: 形態素解析</a></li>
                
                <li><a href="#sec30">30. 形態素解析結果の読み込み</a></li>
                
                <li><a href="#sec31">31. 動詞</a></li>
                
                <li><a href="#sec32">32. 動詞の原形</a></li>
                
                <li><a href="#sec33">33. サ変名詞</a></li>
                
                <li><a href="#sec34">34. 「AのB」</a></li>
                
                <li><a href="#sec35">35. 名詞の連接</a></li>
                
                <li><a href="#sec36">36. 単語の出現頻度</a></li>
                
                <li><a href="#sec37">37. 頻度上位10語</a></li>
                
                <li><a href="#sec38">38. ヒストグラム</a></li>
                
                <li><a href="#sec39">39. Zipfの法則</a></li>
                
              </ul>
            </li>
            
            <li class="dropdown">
              <a href="#ch5" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">第5章<span class="caret"></span></a>
              <ul class="dropdown-menu" role="menu">
                <li><a href="#ch5">第5章: 係り受け解析</a></li>
                
                <li><a href="#sec40">40. 係り受け解析結果の読み込み（形態素）</a></li>
                
                <li><a href="#sec41">41. 係り受け解析結果の読み込み（文節・係り受け）</a></li>
                
                <li><a href="#sec42">42. 係り元と係り先の文節の表示</a></li>
                
                <li><a href="#sec43">43. 名詞を含む文節が動詞を含む文節に係るものを抽出</a></li>
                
                <li><a href="#sec44">44. 係り受け木の可視化</a></li>
                
                <li><a href="#sec45">45. 動詞の格パターンの抽出</a></li>
                
                <li><a href="#sec46">46. 動詞の格フレーム情報の抽出</a></li>
                
                <li><a href="#sec47">47. 機能動詞構文のマイニング</a></li>
                
                <li><a href="#sec48">48. 名詞から根へのパスの抽出</a></li>
                
                <li><a href="#sec49">49. 名詞間の係り受けパスの抽出</a></li>
                
              </ul>
            </li>
            
            <li class="dropdown">
              <a href="#ch6" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">第6章<span class="caret"></span></a>
              <ul class="dropdown-menu" role="menu">
                <li><a href="#ch6">第6章: 英語テキストの処理</a></li>
                
                <li><a href="#sec50">50. 文区切り</a></li>
                
                <li><a href="#sec51">51. 単語の切り出し</a></li>
                
                <li><a href="#sec52">52. ステミング</a></li>
                
                <li><a href="#sec53">53. Tokenization</a></li>
                
                <li><a href="#sec54">54. 品詞タグ付け</a></li>
                
                <li><a href="#sec55">55. 固有表現抽出</a></li>
                
                <li><a href="#sec56">56. 共参照解析</a></li>
                
                <li><a href="#sec57">57. 係り受け解析</a></li>
                
                <li><a href="#sec58">58. タプルの抽出</a></li>
                
                <li><a href="#sec59">59. S式の解析</a></li>
                
              </ul>
            </li>
            
            <li class="dropdown">
              <a href="#ch7" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">第7章<span class="caret"></span></a>
              <ul class="dropdown-menu" role="menu">
                <li><a href="#ch7">第7章: データベース</a></li>
                
                <li><a href="#sec60">60. KVSの構築</a></li>
                
                <li><a href="#sec61">61. KVSの検索</a></li>
                
                <li><a href="#sec62">62. KVS内の反復処理</a></li>
                
                <li><a href="#sec63">63. オブジェクトを値に格納したKVS</a></li>
                
                <li><a href="#sec64">64. MongoDBの構築</a></li>
                
                <li><a href="#sec65">65. MongoDBの検索</a></li>
                
                <li><a href="#sec66">66. 検索件数の取得</a></li>
                
                <li><a href="#sec67">67. 複数のドキュメントの取得</a></li>
                
                <li><a href="#sec68">68. ソート</a></li>
                
                <li><a href="#sec69">69. Webアプリケーションの作成</a></li>
                
              </ul>
            </li>
            
            <li class="dropdown">
              <a href="#ch8" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">第8章<span class="caret"></span></a>
              <ul class="dropdown-menu" role="menu">
                <li><a href="#ch8">第8章: 機械学習</a></li>
                
                <li><a href="#sec70">70. データの入手・整形</a></li>
                
                <li><a href="#sec71">71. ストップワード</a></li>
                
                <li><a href="#sec72">72. 素性抽出</a></li>
                
                <li><a href="#sec73">73. 学習</a></li>
                
                <li><a href="#sec74">74. 予測</a></li>
                
                <li><a href="#sec75">75. 素性の重み</a></li>
                
                <li><a href="#sec76">76. ラベル付け</a></li>
                
                <li><a href="#sec77">77. 正解率の計測</a></li>
                
                <li><a href="#sec78">78. 5分割交差検定</a></li>
                
                <li><a href="#sec79">79. 適合率-再現率グラフの描画</a></li>
                
              </ul>
            </li>
            
            <li class="dropdown">
              <a href="#ch9" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">第9章<span class="caret"></span></a>
              <ul class="dropdown-menu" role="menu">
                <li><a href="#ch9">第9章: ベクトル空間法 (I)</a></li>
                
                <li><a href="#sec80">80. コーパスの整形</a></li>
                
                <li><a href="#sec81">81. 複合語からなる国名への対処</a></li>
                
                <li><a href="#sec82">82. 文脈の抽出</a></li>
                
                <li><a href="#sec83">83. 単語／文脈の頻度の計測</a></li>
                
                <li><a href="#sec84">84. 単語文脈行列の作成</a></li>
                
                <li><a href="#sec85">85. 主成分分析による次元圧縮</a></li>
                
                <li><a href="#sec86">86. 単語ベクトルの表示</a></li>
                
                <li><a href="#sec87">87. 単語の類似度</a></li>
                
                <li><a href="#sec88">88. 類似度の高い単語10件</a></li>
                
                <li><a href="#sec89">89. 加法構成性によるアナロジー</a></li>
                
              </ul>
            </li>
            
            <li class="dropdown">
              <a href="#ch10" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">第10章<span class="caret"></span></a>
              <ul class="dropdown-menu" role="menu">
                <li><a href="#ch10">第10章: ベクトル空間法 (II)</a></li>
                
                <li><a href="#sec90">90. word2vecによる学習</a></li>
                
                <li><a href="#sec91">91. アナロジーデータの準備</a></li>
                
                <li><a href="#sec92">92. アナロジーデータへの適用</a></li>
                
                <li><a href="#sec93">93. アナロジータスクの正解率の計算</a></li>
                
                <li><a href="#sec94">94. WordSimilarity-353での類似度計算</a></li>
                
                <li><a href="#sec95">95. WordSimilarity-353での評価</a></li>
                
                <li><a href="#sec96">96. 国名に関するベクトルの抽出</a></li>
                
                <li><a href="#sec97">97. k-meansクラスタリング</a></li>
                
                <li><a href="#sec98">98. Ward法によるクラスタリング</a></li>
                
                <li><a href="#sec99">99. t-SNEによる可視化</a></li>
                
              </ul>
            </li>
            
            <li><a href="#data">データ</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container">

      <!-- Carousel
           ================================================== -->
      <div class="jumbotron">
        <div class="row">
          <div class="col-md-6">
            <div id="myCarousel" class="carousel slide" data-ride="carousel">
              <!-- Indicators -->
              <ol class="carousel-indicators">
                <li data-target="#myCarousel" data-slide-to="0" class="active"></li>
                <li data-target="#myCarousel" data-slide-to="1"></li>
                <li data-target="#myCarousel" data-slide-to="2"></li>
                <li data-target="#myCarousel" data-slide-to="3"></li>
                <li data-target="#myCarousel" data-slide-to="4"></li>
              </ol>
              <div class="carousel-inner" role="listbox">
                <div class="item active">
                  <img class="img-responsive center-block" src="images/vsm.png" alt="First slide">
                </div>
                <div class="item">
                  <img class="img-responsive center-block" src="images/country.png" alt="Second slide">
                </div>
                <div class="item">
                  <img class="img-responsive center-block" src="images/sentiment.png" alt="Third slide">
                </div>
                <div class="item">
                  <img class="img-responsive center-block" src="images/zipf.png" alt="Fourth slide">
                </div>
                <div class="item">
                  <img class="img-responsive center-block" src="images/neko.png" alt="Fifth slide">
                </div>
              </div>
              <a class="left carousel-control" href="#myCarousel" role="button" data-slide="prev">
                <span class="glyphicon glyphicon-chevron-left" aria-hidden="true"></span>
                <span class="sr-only">Previous</span>
              </a>
              <a class="right carousel-control" href="#myCarousel" role="button" data-slide="next">
                <span class="glyphicon glyphicon-chevron-right" aria-hidden="true"></span>
                <span class="sr-only">Next</span>
              </a>
            </div><!-- /.carousel -->
          </div>
          <div class="col-md-6">
            <h1>言語処理100本ノック 2015</h1>
            <p>言語処理100本ノックは，実践的な課題に取り組みながら，プログラミング，データ分析，研究のスキルを楽しく習得することを目指した問題集です</p>
            <ul>
              <li>実用的でワクワクするような題材を厳選しました</li>
              <li>言語処理に加えて，統計や機械学習などの周辺分野にも親しめます</li>
              <li>研究やデータ分析の進め方，作法，スキルを修得できます</li>
              <li>問題を解くのに必要な<a href="#data">データ・コーパスを配布</a>しています</li>
              <li>言語はPythonを想定していますが，他の言語にも対応しています</li>
            </ul>

            <div class="btn-group">
              <div class="g-plusone" data-size="medium" data-annotation="none"></div>
              <a href="http://b.hatena.ne.jp/entry/http://www.cl.ecei.tohoku.ac.jp/nlp100/" class="hatena-bookmark-button" data-hatena-bookmark-title="言語処理100本ノック 2015" data-hatena-bookmark-layout="standard-balloon" data-hatena-bookmark-lang="ja" title="このエントリーをはてなブックマークに追加"><img src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" alt="このエントリーをはてなブックマークに追加" width="20" height="20" style="border: none;" /></a>

              <a href="https://twitter.com/share" class="twitter-share-button">Tweet</a>
              <div class="fb-like" data-href="http://www.cl.ecei.tohoku.ac.jp/nlp100/" data-layout="button_count" data-action="like" data-show-faces="true" data-share="true"></div>
            </div>
          </div>
        </div>
      </div><!-- /.jumbotron -->

      <!-- Main component for a primary marketing message or call to action -->
      <div class="row">
        <div class="col-md-4">
          <div class="panel panel-default">
            <div class="panel-heading"><h3 class="panel-title">第1章: 準備運動</h3></div>
            <div class="panel-body">
              テキストや文字列を扱う題材に取り組みながら，プログラミング言語のやや高度なトピックを復習します．
            </div>
            <div class="panel-footer">
              文字列, ユニコード, リスト型, 辞書型, 集合型, イテレータ, スライス, 乱数
            </div>
          </div>
        </div>
        <div class="col-md-4">
          <div class="panel panel-default">
            <div class="panel-heading"><h3 class="panel-title">第2章: UNIXコマンドの基礎</h3></div>
            <div class="panel-body">
              研究やデータ分析において便利なUNIXツールを体験します．これらの再実装を通じて，プログラミング能力を高めつつ，既存のツールのエコシステムを体感します．
            </div>
            <div class="panel-footer">
              head, tail, cut, paste, split, sort, uniq, sed, tr, expand
            </div>
          </div>
        </div>
        <div class="col-md-4">
          <div class="panel panel-default">
            <div class="panel-heading"><h3 class="panel-title">第3章: 正規表現</h3></div>
            <div class="panel-body">
              Wikipediaのページのマークアップ記述に正規表現を適用することで，様々な情報・知識を取り出します．
            </div>
            <div class="panel-footer">
              正規表現, JSON, Wikipedia, InfoBox, ウェブサービス
            </div>
          </div>
        </div>
      </div>

      <div class="row">
        <div class="col-md-4">
          <div class="panel panel-default">
            <div class="panel-heading"><h3 class="panel-title">第4章: 形態素解析</h3></div>
            <div class="panel-body">
              夏目漱石の小説『吾輩は猫である』に形態素解析器MeCabを適用し，小説中の単語の統計を求めます．
            </div>
            <div class="panel-footer">
              形態素解析, <a href="http://mecab.googlecode.com/svn/trunk/mecab/doc/index.html">MeCab</a>, 品詞, 出現頻度, Zipfの法則, <a href="http://matplotlib.org/">matplotlib</a>, <a href="http://www.gnuplot.info/">Gnuplot</a>
            </div>
          </div>
        </div>
        <div class="col-md-4">
          <div class="panel panel-default">
            <div class="panel-heading"><h3 class="panel-title">第5章: 係り受け解析</h3></div>
            <div class="panel-body">
              『吾輩は猫である』に係り受け解析器CaboChaを適用し，係り受け木の操作と統語的な分析を体験します．
            </div>
            <div class="panel-footer">
              クラス, 係り受け解析, <a href="https://code.google.com/p/cabocha/">CaboCha</a>, 文節, 係り受け, 格, 機能動詞構文, 係り受けパス, <a href="http://www.graphviz.org/">Graphviz</a>
            </div>
          </div>
        </div>
        <div class="col-md-4">
          <div class="panel panel-default">
            <div class="panel-heading"><h3 class="panel-title">第6章: 英語テキストの処理</h3></div>
            <div class="panel-body">
              Stanford Core NLPを用いた英語のテキスト処理を通じて，自然言語処理の様々な基盤技術を概観します．
            </div>
            <div class="panel-footer">
              <a href="http://nlp.stanford.edu/software/corenlp.shtml">Stanford Core NLP</a>, ステミング, 品詞タグ付け, 固有表現抽出, 共参照解析, 係り受け解析, 句構造解析, S式
            </div>
          </div>
        </div>
      </div>

      <div class="row">
        <div class="col-md-4">
          <div class="panel panel-default">
            <div class="panel-heading"><h3 class="panel-title">第7章: データベース</h3></div>
            <div class="panel-body">
              Key Value Store (KVS) やNoSQLによるデータベースの構築・検索を修得します．また，CGIを用いたデモ・システムを開発します．
            </div>
            <div class="panel-footer">
              <a href="http://leveldb.org/">LevelDB</a>, <a href="http://www.mongodb.org/">MongoDB</a>, JSON, インデックス, 整列, CGI, テンプレートエンジン
            </div>
          </div>
        </div>
        <div class="col-md-4">
          <div class="panel panel-default">
            <div class="panel-heading"><h3 class="panel-title">第8章: 機械学習</h3></div>
            <div class="panel-body">
              評判分析器（ポジネガ分析器）を機械学習で構築します．さらに，手法の評価方法を学びます．
            </div>
            <div class="panel-footer">
              評判分析, ストップワード, 機械学習, 素性, ロジスティック回帰, 交差検定, 適合率, 再現率, <a href="http://scikit-learn.org/">scikit-learn</a>, <a href="http://www.chokkan.org/software/classias">Classias</a>
            </div>
          </div>
        </div>
        <div class="col-md-4">
          <div class="panel panel-default">
            <div class="panel-heading"><h3 class="panel-title">第9章: ベクトル空間法 (I)</h3></div>
            <div class="panel-body">
              大規模なコーパスから単語文脈共起行列を求め，単語の意味を表すベクトルを学習します．その単語ベクトルを用い，単語の類似度やアナロジーを求めます．
            </div>
            <div class="panel-footer">
              ベクトル空間法, 分布仮説, 主成分分析, コサイン類似度, 加法構成性, 複合語, <a href="http://www.numpy.org/">NumPy</a>, <a href="https://code.google.com/p/redsvd/">redsvd</a>
            </div>
          </div>
        </div>
      </div>

      <div class="row">
        <div class="col-md-4">
          <div class="panel panel-default">
            <div class="panel-heading"><h3 class="panel-title">第10章: ベクトル空間法 (II)</h3></div>
            <div class="panel-body">
              word2vecを用いて単語の意味を表すベクトルを学習し，正解データを用いて評価します．さらに，クラスタリングやベクトルの可視化を体験します．
            </div>
            <div class="panel-footer">
              <a href="https://code.google.com/p/word2vec/">word2vec</a>, <a href="http://scikit-learn.org/">scikit-learn</a>, k-meansクラスタリング, 階層型クラスタリング, <a href="http://lvdmaaten.github.io/tsne/">t-SNE</a>
            </div>
          </div>
        </div>
        <div class="col-md-4">
          <div class="panel panel-success">
            <div class="panel-heading"><h3 class="panel-title">2015年版のねらい</h3></div>
            <div class="panel-body">
              2012年の公開以降，様々な研究室・企業で言語処理100本ノックを採用して頂き，様々なご意見を頂戴しました．2015年版では，再配布可能なデータが題材となるように全面的な改訂を行いました．より実践的な題材への置き換えとともに，単語の分散表現に関するテーマを追加しました．
            </div>
          </div>
        </div>
        <div class="col-md-4">
          <div class="panel panel-success">
            <div class="panel-heading"><h3 class="panel-title">お知らせ</h3></div>
            <div class="panel-body">
              言語処理100本ノックに関するお問い合わせ・ご質問は，<a href="http://www.chokkan.org/">岡崎直観</a> (okazaki-at-ecei.tohoku.ac.jp, <a href="https://twitter.com/chokkanorg">@chokkanorg</a>) までお願いします．
              言語処理100本ノックを解くために必要なデータ・コーパスは<a href="data/">こちらからダウンロード</a>できます．以前の問題は，<a href="http://www.cl.ecei.tohoku.ac.jp/index.php?NLP%20100%20Drill%20Exercises">こちらのページ</a>から参照できます．
            </div>
          </div>
        </div>
      </div>

      
      <h2 class="page-header"><a name="ch1"></a>第1章: 準備運動</h2>
      <div>
        

      </div>
      
      <h3><a name="sec00"></a>00. 文字列の逆順</h3>
      <div>
        <p>文字列&quot;stressed&quot;の文字を逆に（末尾から先頭に向かって）並べた文字列を得よ．</p>

      </div>
      
      <h3><a name="sec01"></a>01. 「パタトクカシーー」</h3>
      <div>
        <p>「パタトクカシーー」という文字列の1,3,5,7文字目を取り出して連結した文字列を得よ．</p>

      </div>
      
      <h3><a name="sec02"></a>02. 「パトカー」＋「タクシー」＝「パタトクカシーー」</h3>
      <div>
        <p>「パトカー」＋「タクシー」の文字を先頭から交互に連結して文字列「パタトクカシーー」を得よ．</p>

      </div>
      
      <h3><a name="sec03"></a>03. 円周率</h3>
      <div>
        <p>&quot;Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.&quot;という文を単語に分解し，各単語の（アルファベットの）文字数を先頭から出現順に並べたリストを作成せよ．</p>

      </div>
      
      <h3><a name="sec04"></a>04. 元素記号</h3>
      <div>
        <p>&quot;Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.&quot;という文を単語に分解し，1, 5, 6, 7, 8, 9, 15, 16, 19番目の単語は先頭の1文字，それ以外の単語は先頭に2文字を取り出し，取り出した文字列から単語の位置（先頭から何番目の単語か）への連想配列（辞書型もしくはマップ型）を作成せよ．</p>

      </div>
      
      <h3><a name="sec05"></a>05. n-gram</h3>
      <div>
        <p>与えられたシーケンス（文字列やリストなど）からn-gramを作る関数を作成せよ．この関数を用い，&quot;I am an NLPer&quot;という文から単語bi-gram，文字bi-gramを得よ．</p>

      </div>
      
      <h3><a name="sec06"></a>06. 集合</h3>
      <div>
        <p>&quot;paraparaparadise&quot;と&quot;paragraph&quot;に含まれる文字bi-gramの集合を，それぞれ, XとYとして求め，XとYの和集合，積集合，差集合を求めよ．さらに，'se'というbi-gramがXおよびYに含まれるかどうかを調べよ．</p>

      </div>
      
      <h3><a name="sec07"></a>07. テンプレートによる文生成</h3>
      <div>
        <p>引数x, y, zを受け取り「x時のyはz」という文字列を返す関数を実装せよ．さらに，x=12, y=&quot;気温&quot;, z=22.4として，実行結果を確認せよ．</p>

      </div>
      
      <h3><a name="sec08"></a>08. 暗号文</h3>
      <div>
        <p>与えられた文字列の各文字を，以下の仕様で変換する関数cipherを実装せよ．</p>
<ul>
<li>英小文字ならば(219 - 文字コード)の文字に置換</li>
<li>その他の文字はそのまま出力</li>
</ul>
<p>この関数を用い，英語のメッセージを暗号化・復号化せよ．</p>

      </div>
      
      <h3><a name="sec09"></a>09. Typoglycemia</h3>
      <div>
        <p>スペースで区切られた単語列に対して，各単語の先頭と末尾の文字は残し，それ以外の文字の順序をランダムに並び替えるプログラムを作成せよ．ただし，長さが４以下の単語は並び替えないこととする．適当な英語の文（例えば&quot;I couldn't believe that I could actually understand what I was reading : the phenomenal power of the human mind .&quot;）を与え，その実行結果を確認せよ．</p>

      </div>
      
      
      <h2 class="page-header"><a name="ch2"></a>第2章: UNIXコマンドの基礎</h2>
      <div>
        <p><a href="data/hightemp.txt">hightemp.txt</a>は，日本の最高気温の記録を「都道府県」「地点」「℃」「日」のタブ区切り形式で格納したファイルである．以下の処理を行うプログラムを作成し，<a href="data/hightemp.txt">hightemp.txt</a>を入力ファイルとして実行せよ．さらに，同様の処理をUNIXコマンドでも実行し，プログラムの実行結果を確認せよ．</p>

      </div>
      
      <h3><a name="sec10"></a>10. 行数のカウント</h3>
      <div>
        <p>行数をカウントせよ．確認にはwcコマンドを用いよ．</p>

      </div>
      
      <h3><a name="sec11"></a>11. タブをスペースに置換</h3>
      <div>
        <p>タブ1文字につきスペース1文字に置換せよ．確認にはsedコマンド，trコマンド，もしくはexpandコマンドを用いよ．</p>

      </div>
      
      <h3><a name="sec12"></a>12. 1列目をcol1.txtに，2列目をcol2.txtに保存</h3>
      <div>
        <p>各行の1列目だけを抜き出したものをcol1.txtに，2列目だけを抜き出したものをcol2.txtとしてファイルに保存せよ．確認にはcutコマンドを用いよ．</p>

      </div>
      
      <h3><a name="sec13"></a>13. col1.txtとcol2.txtをマージ</h3>
      <div>
        <p>12で作ったcol1.txtとcol2.txtを結合し，元のファイルの1列目と2列目をタブ区切りで並べたテキストファイルを作成せよ．確認にはpasteコマンドを用いよ．</p>

      </div>
      
      <h3><a name="sec14"></a>14. 先頭からN行を出力</h3>
      <div>
        <p>自然数Nをコマンドライン引数などの手段で受け取り，入力のうち先頭のN行だけを表示せよ．確認にはheadコマンドを用いよ．</p>

      </div>
      
      <h3><a name="sec15"></a>15. 末尾のN行を出力</h3>
      <div>
        <p>自然数Nをコマンドライン引数などの手段で受け取り，入力のうち末尾のN行だけを表示せよ．確認にはtailコマンドを用いよ．</p>

      </div>
      
      <h3><a name="sec16"></a>16. ファイルをN分割する</h3>
      <div>
        <p>自然数Nをコマンドライン引数などの手段で受け取り，入力のファイルを行単位でN分割せよ．同様の処理をsplitコマンドで実現せよ．</p>

      </div>
      
      <h3><a name="sec17"></a>17. １列目の文字列の異なり</h3>
      <div>
        <p>1列目の文字列の種類（異なる文字列の集合）を求めよ．確認にはsort, uniqコマンドを用いよ．</p>

      </div>
      
      <h3><a name="sec18"></a>18. 各行を3コラム目の数値の降順にソート</h3>
      <div>
        <p>各行を3コラム目の数値の逆順で整列せよ（注意: 各行の内容は変更せずに並び替えよ）．確認にはsortコマンドを用いよ（この問題はコマンドで実行した時の結果と合わなくてもよい）．</p>

      </div>
      
      <h3><a name="sec19"></a>19. 各行の1コラム目の文字列の出現頻度を求め，出現頻度の高い順に並べる</h3>
      <div>
        <p>各行の1列目の文字列の出現頻度を求め，その高い順に並べて表示せよ．確認にはcut, uniq, sortコマンドを用いよ．</p>

      </div>
      
      
      <h2 class="page-header"><a name="ch3"></a>第3章: 正規表現</h2>
      <div>
        <p>Wikipediaの記事を以下のフォーマットで書き出したファイル<a href="data/jawiki-country.json.gz">jawiki-country.json.gz</a>がある．</p>
<ul>
<li>1行に1記事の情報がJSON形式で格納される</li>
<li>各行には記事名が&quot;title&quot;キーに，記事本文が&quot;text&quot;キーの辞書オブジェクトに格納され，そのオブジェクトがJSON形式で書き出される</li>
<li>ファイル全体はgzipで圧縮される</li>
</ul>
<p>以下の処理を行うプログラムを作成せよ．</p>

      </div>
      
      <h3><a name="sec20"></a>20. JSONデータの読み込み</h3>
      <div>
        <p>Wikipedia記事のJSONファイルを読み込み，「イギリス」に関する記事本文を表示せよ．問題21-29では，ここで抽出した記事本文に対して実行せよ．</p>

      </div>
      
      <h3><a name="sec21"></a>21. カテゴリ名を含む行を抽出</h3>
      <div>
        <p>記事中でカテゴリ名を宣言している行を抽出せよ．</p>

      </div>
      
      <h3><a name="sec22"></a>22. カテゴリ名の抽出</h3>
      <div>
        <p>記事のカテゴリ名を（行単位ではなく名前で）抽出せよ．</p>

      </div>
      
      <h3><a name="sec23"></a>23. セクション構造</h3>
      <div>
        <p>記事中に含まれるセクション名とそのレベル（例えば&quot;== セクション名 ==&quot;なら1）を表示せよ．</p>

      </div>
      
      <h3><a name="sec24"></a>24. ファイル参照の抽出</h3>
      <div>
        <p>記事から参照されているメディアファイルをすべて抜き出せ．</p>

      </div>
      
      <h3><a name="sec25"></a>25. テンプレートの抽出</h3>
      <div>
        <p>記事中に含まれる「基礎情報」テンプレートのフィールド名と値を抽出し，辞書オブジェクトとして格納せよ．</p>

      </div>
      
      <h3><a name="sec26"></a>26. 強調マークアップの除去</h3>
      <div>
        <p>25の処理時に，テンプレートの値からMediaWikiの強調マークアップ（弱い強調，強調，強い強調のすべて）を除去してテキストに変換せよ（参考: <a href="http://ja.wikipedia.org/wiki/Help:%E6%97%A9%E8%A6%8B%E8%A1%A8">マークアップ早見表</a>）．</p>

      </div>
      
      <h3><a name="sec27"></a>27. 内部リンクの除去</h3>
      <div>
        <p>26の処理に加えて，テンプレートの値からMediaWikiの内部リンクマークアップを除去し，テキストに変換せよ（参考: <a href="http://ja.wikipedia.org/wiki/Help:%E6%97%A9%E8%A6%8B%E8%A1%A8">マークアップ早見表</a>）．</p>

      </div>
      
      <h3><a name="sec28"></a>28. MediaWikiマークアップの除去</h3>
      <div>
        <p>27の処理に加えて，テンプレートの値からMediaWikiマークアップを可能な限り除去し，国の基本情報を整形せよ．</p>

      </div>
      
      <h3><a name="sec29"></a>29. 国旗画像のURLを取得する</h3>
      <div>
        <p>テンプレートの内容を利用し，国旗画像のURLを取得せよ．（ヒント: <a href="http://www.mediawiki.org/wiki/API:Main_page/ja">MediaWiki API</a>の<a href="http://www.mediawiki.org/wiki/API:Properties/ja#imageinfo_.2F_ii">imageinfo</a>を呼び出して，ファイル参照をURLに変換すればよい）</p>

      </div>
      
      
      <h2 class="page-header"><a name="ch4"></a>第4章: 形態素解析</h2>
      <div>
        <p>夏目漱石の小説『吾輩は猫である』の文章（<a href="data/neko.txt">neko.txt</a>）をMeCabを使って形態素解析し，その結果をneko.txt.mecabというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．</p>
<p>なお，問題37, 38, 39は<a href="http://matplotlib.org/">matplotlib</a>もしくは<a href="http://www.gnuplot.info/">Gnuplot</a>を用いるとよい．</p>

      </div>
      
      <h3><a name="sec30"></a>30. 形態素解析結果の読み込み</h3>
      <div>
        <p>形態素解析結果（neko.txt.mecab）を読み込むプログラムを実装せよ．ただし，各形態素は表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をキーとするマッピング型に格納し，1文を形態素（マッピング型）のリストとして表現せよ．第4章の残りの問題では，ここで作ったプログラムを活用せよ．</p>

      </div>
      
      <h3><a name="sec31"></a>31. 動詞</h3>
      <div>
        <p>動詞の表層形をすべて抽出せよ．</p>

      </div>
      
      <h3><a name="sec32"></a>32. 動詞の原形</h3>
      <div>
        <p>動詞の原形をすべて抽出せよ．</p>

      </div>
      
      <h3><a name="sec33"></a>33. サ変名詞</h3>
      <div>
        <p>サ変接続の名詞をすべて抽出せよ．</p>

      </div>
      
      <h3><a name="sec34"></a>34. 「AのB」</h3>
      <div>
        <p>2つの名詞が「の」で連結されている名詞句を抽出せよ．</p>

      </div>
      
      <h3><a name="sec35"></a>35. 名詞の連接</h3>
      <div>
        <p>名詞の連接（連続して出現する名詞）を最長一致で抽出せよ．</p>

      </div>
      
      <h3><a name="sec36"></a>36. 単語の出現頻度</h3>
      <div>
        <p>文章中に出現する単語とその出現頻度を求め，出現頻度の高い順に並べよ．</p>

      </div>
      
      <h3><a name="sec37"></a>37. 頻度上位10語</h3>
      <div>
        <p>出現頻度が高い10語とその出現頻度をグラフ（例えば棒グラフなど）で表示せよ．</p>

      </div>
      
      <h3><a name="sec38"></a>38. ヒストグラム</h3>
      <div>
        <p>単語の出現頻度のヒストグラム（横軸に出現頻度，縦軸に出現頻度をとる単語の種類数を棒グラフで表したもの）を描け．</p>

      </div>
      
      <h3><a name="sec39"></a>39. Zipfの法則</h3>
      <div>
        <p>単語の出現頻度順位を横軸，その出現頻度を縦軸として，両対数グラフをプロットせよ．</p>

      </div>
      
      
      <h2 class="page-header"><a name="ch5"></a>第5章: 係り受け解析</h2>
      <div>
        <p>夏目漱石の小説『吾輩は猫である』の文章（<a href="data/neko.txt">neko.txt</a>）をCaboChaを使って係り受け解析し，その結果をneko.txt.cabochaというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．</p>

      </div>
      
      <h3><a name="sec40"></a>40. 係り受け解析結果の読み込み（形態素）</h3>
      <div>
        <p>形態素を表すクラス<code>Morph</code>を実装せよ．このクラスは表層形（<code>surface</code>），基本形（<code>base</code>），品詞（<code>pos</code>），品詞細分類1（<code>pos1</code>）をメンバ変数に持つこととする．さらに，CaboChaの解析結果（neko.txt.cabocha）を読み込み，各文を<code>Morph</code>オブジェクトのリストとして表現し，3文目の形態素列を表示せよ．</p>

      </div>
      
      <h3><a name="sec41"></a>41. 係り受け解析結果の読み込み（文節・係り受け）</h3>
      <div>
        <p>40に加えて，文節を表すクラス<code>Chunk</code>を実装せよ．このクラスは形態素（<code>Morph</code>オブジェクト）のリスト（<code>morphs</code>），係り先文節インデックス番号（<code>dst</code>），係り元文節インデックス番号のリスト（<code>srcs</code>）をメンバ変数に持つこととする．さらに，入力テキストのCaboChaの解析結果を読み込み，１文を<code>Chunk</code>オブジェクトのリストとして表現し，8文目の文節の文字列と係り先を表示せよ．第5章の残りの問題では，ここで作ったプログラムを活用せよ．</p>

      </div>
      
      <h3><a name="sec42"></a>42. 係り元と係り先の文節の表示</h3>
      <div>
        <p>係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．</p>

      </div>
      
      <h3><a name="sec43"></a>43. 名詞を含む文節が動詞を含む文節に係るものを抽出</h3>
      <div>
        <p>名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．</p>

      </div>
      
      <h3><a name="sec44"></a>44. 係り受け木の可視化</h3>
      <div>
        <p>与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，係り受け木を<a href="http://ja.wikipedia.org/wiki/DOT%E8%A8%80%E8%AA%9E">DOT言語</a>に変換し，<a href="http://www.graphviz.org/">Graphviz</a>を用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，<a href="https://code.google.com/p/pydot/">pydot</a>を使うとよい．</p>

      </div>
      
      <h3><a name="sec45"></a>45. 動詞の格パターンの抽出</h3>
      <div>
        <p>今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．</p>
<ul>
<li>動詞を含む文節において，最左の動詞の基本形を述語とする</li>
<li>述語に係る助詞を格とする</li>
<li>述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる</li>
</ul>
<p>「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．</p>
<pre><code>始める  で
見る    は を</code></pre>
<p>このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．</p>
<ul>
<li>コーパス中で頻出する述語と格パターンの組み合わせ</li>
<li>「する」「見る」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）</li>
</ul>

      </div>
      
      <h3><a name="sec46"></a>46. 動詞の格フレーム情報の抽出</h3>
      <div>
        <p>45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．</p>
<ul>
<li>項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）</li>
<li>述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる</li>
</ul>
<p>「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．</p>
<pre><code>始める  で      ここで
見る    は を   吾輩は ものを</code></pre>

      </div>
      
      <h3><a name="sec47"></a>47. 機能動詞構文のマイニング</h3>
      <div>
        <p>動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．</p>
<ul>
<li>「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする</li>
<li>述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる</li>
<li>述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる</li>
<li>述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）</li>
</ul>
<p>例えば「別段くるにも及ばんさと、主人は手紙に返事をする。」という文から，以下の出力が得られるはずである．</p>
<pre><code>返事をする      と に は        及ばんさと 手紙に 主人は</code></pre>
<p>このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．</p>
<ul>
<li>コーパス中で頻出する述語（サ変接続名詞+を+動詞）</li>
<li>コーパス中で頻出する述語と助詞パターン</li>
</ul>

      </div>
      
      <h3><a name="sec48"></a>48. 名詞から根へのパスの抽出</h3>
      <div>
        <p>文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．</p>
<ul>
<li>各文節は（表層形の）形態素列で表現する</li>
<li>パスの開始文節から終了文節に至るまで，各文節の表現を&quot;<code>-&gt;</code>&quot;で連結する</li>
</ul>
<p>「吾輩はここで始めて人間というものを見た」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．</p>
<pre><code>吾輩は -&gt; 見た
ここで -&gt; 始めて -&gt; 人間という -&gt; ものを -&gt; 見た
人間という -&gt; ものを -&gt; 見た
ものを -&gt; 見た</code></pre>

      </div>
      
      <h3><a name="sec49"></a>49. 名詞間の係り受けパスの抽出</h3>
      <div>
        <p>文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号が<span class="math">\(i\)</span>と<span class="math">\(j\)</span>（<span class="math">\(i &lt; j\)</span>）のとき，係り受けパスは以下の仕様を満たすものとする．</p>
<ul>
<li>問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を&quot;<code>-&gt;</code>&quot;で連結して表現する</li>
<li>文節<span class="math">\(i\)</span>と<span class="math">\(j\)</span>に含まれる名詞句はそれぞれ，XとYに置換する</li>
</ul>
<p>また，係り受けパスの形状は，以下の2通りが考えられる．</p>
<ul>
<li>文節<span class="math">\(i\)</span>から構文木の根に至る経路上に文節<span class="math">\(j\)</span>が存在する場合: 文節<span class="math">\(i\)</span>から文節<span class="math">\(j\)</span>のパスを表示</li>
<li>上記以外で，文節<span class="math">\(i\)</span>と文節<span class="math">\(j\)</span>から構文木の根に至る経路上で共通の文節<span class="math">\(k\)</span>で交わる場合: 文節<span class="math">\(i\)</span>から文節<span class="math">\(k\)</span>に至る直前のパスと文節<span class="math">\(j\)</span>から文節<span class="math">\(k\)</span>に至る直前までのパス，文節<span class="math">\(k\)</span>の内容を&quot;<code>|</code>&quot;で連結して表示</li>
</ul>
<p>例えば，「吾輩はここで始めて人間というものを見た。」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．</p>
<pre><code>Xは | Yで -&gt; 始めて -&gt; 人間という -&gt; ものを | 見た
Xは | Yという -&gt; ものを | 見た
Xは | Yを | 見た
Xで -&gt; 始めて -&gt; Y
Xで -&gt; 始めて -&gt; 人間という -&gt; Y
Xという -&gt; Y</code></pre>

      </div>
      
      
      <h2 class="page-header"><a name="ch6"></a>第6章: 英語テキストの処理</h2>
      <div>
        <p>英語のテキスト（<a href="data/nlp.txt">nlp.txt</a>）に対して，以下の処理を実行せよ．</p>

      </div>
      
      <h3><a name="sec50"></a>50. 文区切り</h3>
      <div>
        <p>(. or ; or : or ? or !) → 空白文字 → 英大文字というパターンを文の区切りと見なし，入力された文書を1行1文の形式で出力せよ．</p>

      </div>
      
      <h3><a name="sec51"></a>51. 単語の切り出し</h3>
      <div>
        <p>空白を単語の区切りとみなし，50の出力を入力として受け取り，1行1単語の形式で出力せよ．ただし，文の終端では空行を出力せよ．</p>

      </div>
      
      <h3><a name="sec52"></a>52. ステミング</h3>
      <div>
        <p>51の出力を入力として受け取り，Porterのステミングアルゴリズムを適用し，単語と語幹をタブ区切り形式で出力せよ． Pythonでは，Porterのステミングアルゴリズムの実装として<a href="https://pypi.python.org/pypi/stemming">stemming</a>モジュールを利用するとよい．</p>

      </div>
      
      <h3><a name="sec53"></a>53. Tokenization</h3>
      <div>
        <p><a href="http://nlp.stanford.edu/software/corenlp.shtml">Stanford Core NLP</a>を用い，入力テキストの解析結果をXML形式で得よ．また，このXMLファイルを読み込み，入力テキストを1行1単語の形式で出力せよ．</p>

      </div>
      
      <h3><a name="sec54"></a>54. 品詞タグ付け</h3>
      <div>
        <p>Stanford Core NLPの解析結果XMLを読み込み，単語，レンマ，品詞をタブ区切り形式で出力せよ．</p>

      </div>
      
      <h3><a name="sec55"></a>55. 固有表現抽出</h3>
      <div>
        <p>入力文中の人名をすべて抜き出せ．</p>

      </div>
      
      <h3><a name="sec56"></a>56. 共参照解析</h3>
      <div>
        <p>Stanford Core NLPの共参照解析の結果に基づき，文中の参照表現（mention）を代表参照表現（representative mention）に置換せよ．ただし，置換するときは，「代表参照表現（参照表現）」のように，元の参照表現が分かるように配慮せよ．</p>

      </div>
      
      <h3><a name="sec57"></a>57. 係り受け解析</h3>
      <div>
        <p>Stanford Core NLPの係り受け解析の結果（collapsed-dependencies）を有向グラフとして可視化せよ．可視化には，係り受け木を<a href="http://ja.wikipedia.org/wiki/DOT%E8%A8%80%E8%AA%9E">DOT言語</a>に変換し，<a href="http://www.graphviz.org/">Graphviz</a>を用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，<a href="https://code.google.com/p/pydot/">pydot</a>を使うとよい．</p>

      </div>
      
      <h3><a name="sec58"></a>58. タプルの抽出</h3>
      <div>
        <p>Stanford Core NLPの係り受け解析の結果（collapsed-dependencies）に基づき，「主語 述語 目的語」の組をタブ区切り形式で出力せよ．ただし，主語，述語，目的語の定義は以下を参考にせよ．</p>
<ul>
<li>述語: nsubj関係とdobj関係の子（dependant）を持つ単語</li>
<li>主語: 述語からnsubj関係にある子（dependent）</li>
<li>目的語: 述語からdobj関係にある子（dependent）</li>
</ul>

      </div>
      
      <h3><a name="sec59"></a>59. S式の解析</h3>
      <div>
        <p>Stanford Core NLPの句構造解析の結果（S式）を読み込み，文中のすべての名詞句（NP）を表示せよ．入れ子になっている名詞句もすべて表示すること．</p>

      </div>
      
      
      <h2 class="page-header"><a name="ch7"></a>第7章: データベース</h2>
      <div>
        <p><a href="data/artist.json.gz">artist.json.gz</a>は，オープンな音楽データベース<a href="https://musicbrainz.org/">MusicBrainz</a>の中で，アーティストに関するものをJSON形式に変換し，gzip形式で圧縮したファイルである．このファイルには，1アーティストに関する情報が1行にJSON形式で格納されている．JSON形式の概要は以下の通りである．</p>
<table class="table table-striped">
<thead>
<tr class="header">
<th align="left">フィールド</th>
<th align="left">型</th>
<th align="left">内容</th>
<th align="left">例</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">id</td>
<td align="left">ユニーク識別子</td>
<td align="left">整数</td>
<td align="left">20660</td>
</tr>
<tr class="even">
<td align="left">gid</td>
<td align="left">グローバル識別子</td>
<td align="left">文字列</td>
<td align="left">&quot;ecf9f3a3-35e9-4c58-acaa-e707fba45060&quot;</td>
</tr>
<tr class="odd">
<td align="left">name</td>
<td align="left">アーティスト名</td>
<td align="left">文字列</td>
<td align="left">&quot;Oasis&quot;</td>
</tr>
<tr class="even">
<td align="left">sort_name</td>
<td align="left">アーティスト名（辞書順整列用）</td>
<td align="left">文字列</td>
<td align="left">&quot;Oasis&quot;</td>
</tr>
<tr class="odd">
<td align="left">area</td>
<td align="left">活動場所</td>
<td align="left">文字列</td>
<td align="left">&quot;United Kingdom&quot;</td>
</tr>
<tr class="even">
<td align="left">aliases</td>
<td align="left">別名</td>
<td align="left">辞書オブジェクトのリスト</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">aliases[].name</td>
<td align="left">別名</td>
<td align="left">文字列</td>
<td align="left">&quot;オアシス&quot;</td>
</tr>
<tr class="even">
<td align="left">aliases[].sort_name</td>
<td align="left">別名（整列用）</td>
<td align="left">文字列</td>
<td align="left">&quot;オアシス&quot;</td>
</tr>
<tr class="odd">
<td align="left">begin</td>
<td align="left">活動開始日</td>
<td align="left">辞書</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">begin.year</td>
<td align="left">活動開始年</td>
<td align="left">整数</td>
<td align="left">1991</td>
</tr>
<tr class="odd">
<td align="left">begin.month</td>
<td align="left">活動開始月</td>
<td align="left">整数</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">begin.date</td>
<td align="left">活動開始日</td>
<td align="left">整数</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">end</td>
<td align="left">活動終了日</td>
<td align="left">辞書</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">end.year</td>
<td align="left">活動終了年</td>
<td align="left">整数</td>
<td align="left">2009</td>
</tr>
<tr class="odd">
<td align="left">end.month</td>
<td align="left">活動終了月</td>
<td align="left">整数</td>
<td align="left">8</td>
</tr>
<tr class="even">
<td align="left">end.date</td>
<td align="left">活動終了日</td>
<td align="left">整数</td>
<td align="left">28</td>
</tr>
<tr class="odd">
<td align="left">tags</td>
<td align="left">タグ</td>
<td align="left">辞書オブジェクトのリスト</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">tags[].count</td>
<td align="left">タグ付けされた回数</td>
<td align="left">整数</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">tags[].value</td>
<td align="left">タグ内容</td>
<td align="left">文字列</td>
<td align="left">&quot;rock&quot;</td>
</tr>
<tr class="even">
<td align="left">rating</td>
<td align="left">レーティング</td>
<td align="left">辞書オブジェクト</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">rating.count</td>
<td align="left">レーティングの投票数</td>
<td align="left">整数</td>
<td align="left">13</td>
</tr>
<tr class="even">
<td align="left">rating.value</td>
<td align="left">レーティングの値（平均値）</td>
<td align="left">整数</td>
<td align="left">86</td>
</tr>
</tbody>
</table>
<p>artist.json.gzのデータをKey-Value-Store (KVS) およびドキュメント志向型データベースに格納・検索することを考える．KVSとしては，<a href="http://leveldb.org/">LevelDB</a>，<a href="http://redis.io/">Redis</a>，<a href="http://fallabs.com/kyotocabinet/">KyotoCabinet</a>等を用いよ．ドキュメント志向型データベースとして，<a href="http://www.mongodb.org/">MongoDB</a>を採用したが，<a href="http://couchdb.apache.org/">CouchDB</a>や<a href="http://rethinkdb.com/">RethinkDB</a>等を用いてもよい．</p>

      </div>
      
      <h3><a name="sec60"></a>60. KVSの構築</h3>
      <div>
        <p>Key-Value-Store (KVS) を用い，アーティスト名（name）から活動場所（area）を検索するためのデータベースを構築せよ．</p>

      </div>
      
      <h3><a name="sec61"></a>61. KVSの検索</h3>
      <div>
        <p>60で構築したデータベースを用い，特定の（指定された）アーティストの活動場所を取得せよ．</p>

      </div>
      
      <h3><a name="sec62"></a>62. KVS内の反復処理</h3>
      <div>
        <p>60で構築したデータベースを用い，活動場所が「Japan」となっているアーティスト数を求めよ．</p>

      </div>
      
      <h3><a name="sec63"></a>63. オブジェクトを値に格納したKVS</h3>
      <div>
        <p>KVSを用い，アーティスト名（name）からタグと被タグ数（タグ付けされた回数）のリストを検索するためのデータベースを構築せよ．さらに，ここで構築したデータベースを用い，アーティスト名からタグと被タグ数を検索せよ．</p>

      </div>
      
      <h3><a name="sec64"></a>64. MongoDBの構築</h3>
      <div>
        <p>アーティスト情報（artist.json.gz）をデータベースに登録せよ．さらに，次のフィールドでインデックスを作成せよ: name, aliases.name, tags.value, rating.value</p>

      </div>
      
      <h3><a name="sec65"></a>65. MongoDBの検索</h3>
      <div>
        <p>MongoDBのインタラクティブシェルを用いて，&quot;Queen&quot;というアーティストに関する情報を取得せよ．さらに，これと同様の処理を行うプログラムを実装せよ．</p>

      </div>
      
      <h3><a name="sec66"></a>66. 検索件数の取得</h3>
      <div>
        <p>MongoDBのインタラクティブシェルを用いて，活動場所が「Japan」となっているアーティスト数を求めよ．</p>

      </div>
      
      <h3><a name="sec67"></a>67. 複数のドキュメントの取得</h3>
      <div>
        <p>特定の（指定した）別名を持つアーティストを検索せよ．</p>

      </div>
      
      <h3><a name="sec68"></a>68. ソート</h3>
      <div>
        <p>&quot;dance&quot;というタグを付与されたアーティストの中でレーティングの投票数が多いアーティスト・トップ10を求めよ．</p>

      </div>
      
      <h3><a name="sec69"></a>69. Webアプリケーションの作成</h3>
      <div>
        <p>ユーザから入力された検索条件に合致するアーティストの情報を表示するWebアプリケーションを作成せよ．アーティスト名，アーティストの別名，タグ等で検索条件を指定し，アーティスト情報のリストをレーティングの高い順などで整列して表示せよ．</p>

      </div>
      
      
      <h2 class="page-header"><a name="ch8"></a>第8章: 機械学習</h2>
      <div>
        <p>本章では，Bo Pang氏とLillian Lee氏が公開している<a href="http://www.cs.cornell.edu/people/pabo/movie-review-data/">Movie Review Data</a>の<a href="http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.README.1.0.txt">sentence polarity dataset v1.0</a>を用い，文を肯定的（ポジティブ）もしくは否定的（ネガティブ）に分類するタスク（極性分析）に取り組む．</p>

      </div>
      
      <h3><a name="sec70"></a>70. データの入手・整形</h3>
      <div>
        <p><a href="http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz">文に関する極性分析の正解データ</a>を用い，以下の要領で正解データ（sentiment.txt）を作成せよ．</p>
<ol style="list-style-type: decimal">
<li>rt-polarity.posの各行の先頭に&quot;+1 &quot;という文字列を追加する（極性ラベル&quot;+1&quot;とスペースに続けて肯定的な文の内容が続く）</li>
<li>rt-polarity.negの各行の先頭に&quot;-1 &quot;という文字列を追加する（極性ラベル&quot;-1&quot;とスペースに続けて否定的な文の内容が続く）</li>
<li>上述1と2の内容を結合（concatenate）し，行をランダムに並び替える</li>
</ol>
<p>sentiment.txtを作成したら，正例（肯定的な文）の数と負例（否定的な文）の数を確認せよ．</p>

      </div>
      
      <h3><a name="sec71"></a>71. ストップワード</h3>
      <div>
        <p>英語のストップワードのリスト（ストップリスト）を適当に作成せよ．さらに，引数に与えられた単語（文字列）がストップリストに含まれている場合は真，それ以外は偽を返す関数を実装せよ．さらに，その関数に対するテストを記述せよ．</p>

      </div>
      
      <h3><a name="sec72"></a>72. 素性抽出</h3>
      <div>
        <p>極性分析に有用そうな素性を各自で設計し，学習データから素性を抽出せよ．素性としては，レビューからストップワードを除去し，各単語をステミング処理したものが最低限のベースラインとなるであろう．</p>

      </div>
      
      <h3><a name="sec73"></a>73. 学習</h3>
      <div>
        <p>72で抽出した素性を用いて，ロジスティック回帰モデルを学習せよ．</p>

      </div>
      
      <h3><a name="sec74"></a>74. 予測</h3>
      <div>
        <p>73で学習したロジスティック回帰モデルを用い，与えられた文の極性ラベル（正例なら&quot;+1&quot;，負例なら&quot;-1&quot;）と，その予測確率を計算するプログラムを実装せよ．</p>

      </div>
      
      <h3><a name="sec75"></a>75. 素性の重み</h3>
      <div>
        <p>73で学習したロジスティック回帰モデルの中で，重みの高い素性トップ10と，重みの低い素性トップ10を確認せよ．</p>

      </div>
      
      <h3><a name="sec76"></a>76. ラベル付け</h3>
      <div>
        <p>学習データに対してロジスティック回帰モデルを適用し，正解のラベル，予測されたラベル，予測確率をタブ区切り形式で出力せよ．</p>

      </div>
      
      <h3><a name="sec77"></a>77. 正解率の計測</h3>
      <div>
        <p>76の出力を受け取り，予測の正解率，正例に関する適合率，再現率，F1スコアを求めるプログラムを作成せよ．</p>

      </div>
      
      <h3><a name="sec78"></a>78. 5分割交差検定</h3>
      <div>
        <p>76-77の実験では，学習に用いた事例を評価にも用いたため，正当な評価とは言えない．すなわち，分類器が訓練事例を丸暗記する際の性能を評価しており，モデルの汎化性能を測定していない．そこで，5分割交差検定により，極性分類の正解率，適合率，再現率，F1スコアを求めよ．</p>

      </div>
      
      <h3><a name="sec79"></a>79. 適合率-再現率グラフの描画</h3>
      <div>
        <p>ロジスティック回帰モデルの分類の閾値を変化させることで，適合率-再現率グラフを描画せよ．</p>

      </div>
      
      
      <h2 class="page-header"><a name="ch9"></a>第9章: ベクトル空間法 (I)</h2>
      <div>
        <p><a href="data/enwiki-20150112-400-r10-105752.txt.bz2">enwiki-20150112-400-r10-105752.txt.bz2</a>は，2015年1月12日時点の英語のWikipedia記事のうち，約400語以上で構成される記事の中から，ランダムに1/10サンプリングした105,752記事のテキストをbzip2形式で圧縮したものである．このテキストをコーパスとして，単語の意味を表すベクトル（分散表現）を学習したい．第9章の前半では，コーパスから作成した単語文脈共起行列に主成分分析を適用し，単語ベクトルを学習する過程を，いくつかの処理に分けて実装する．第9章の後半では，学習で得られた単語ベクトル（300次元）を用い，単語の類似度計算やアナロジー（類推）を行う．</p>
<p>なお，問題83を素直に実装すると，大量（約7GB）の主記憶が必要になる． メモリが不足する場合は，処理を工夫するか，1/100サンプリングのコーパス<a href="data/enwiki-20150112-400-r100-10576.txt.bz2">enwiki-20150112-400-r100-10576.txt.bz2</a>を用いよ．</p>

      </div>
      
      <h3><a name="sec80"></a>80. コーパスの整形</h3>
      <div>
        <p>文を単語列に変換する最も単純な方法は，空白文字で単語に区切ることである． ただ，この方法では文末のピリオドや括弧などの記号が単語に含まれてしまう． そこで，コーパスの各行のテキストを空白文字でトークンのリストに分割した後，各トークンに以下の処理を施し，単語から記号を除去せよ．</p>
<ul>
<li>トークンの先頭と末尾に出現する次の文字を削除: <code>.,!?;:()[]'&quot;</code></li>
<li>空文字列となったトークンは削除</li>
</ul>
<p>以上の処理を適用した後，トークンをスペースで連結してファイルに保存せよ．</p>

      </div>
      
      <h3><a name="sec81"></a>81. 複合語からなる国名への対処</h3>
      <div>
        <p>英語では，複数の語の連接が意味を成すことがある．例えば，アメリカ合衆国は&quot;United States&quot;，イギリスは&quot;United Kingdom&quot;と表現されるが，&quot;United&quot;や&quot;States&quot;，&quot;Kingdom&quot;という単語だけでは，指し示している概念・実体が曖昧である．そこで，コーパス中に含まれる複合語を認識し，複合語を1語として扱うことで，複合語の意味を推定したい．しかしながら，複合語を正確に認定するのは大変むずかしいので，ここでは複合語からなる国名を認定したい．</p>
<p>インターネット上から国名リストを各自で入手し，80のコーパス中に出現する複合語の国名に関して，スペースをアンダーバーに置換せよ．例えば，&quot;United States&quot;は&quot;United_States&quot;，&quot;Isle of Man&quot;は&quot;Isle_of_Man&quot;になるはずである．</p>

      </div>
      
      <h3><a name="sec82"></a>82. 文脈の抽出</h3>
      <div>
        <p>81で作成したコーパス中に出現するすべての単語<span class="math">\(t\)</span>に関して，単語<span class="math">\(t\)</span>と文脈語<span class="math">\(c\)</span>のペアをタブ区切り形式ですべて書き出せ．ただし，文脈語の定義は次の通りとする．</p>
<ul>
<li>ある単語<span class="math">\(t\)</span>の前後<span class="math">\(d\)</span>単語を文脈語<span class="math">\(c\)</span>として抽出する（ただし，文脈語に単語<span class="math">\(t\)</span>そのものは含まない）</li>
<li>単語<span class="math">\(t\)</span>を選ぶ度に，文脈幅<span class="math">\(d\)</span>は<span class="math">\(\{1, 2, 3, 4, 5\}\)</span>の範囲でランダムに決める．</li>
</ul>

      </div>
      
      <h3><a name="sec83"></a>83. 単語／文脈の頻度の計測</h3>
      <div>
        <p>82の出力を利用し，以下の出現分布，および定数を求めよ．</p>
<ul>
<li><span class="math">\(f(t,c)\)</span>: 単語<span class="math">\(t\)</span>と文脈語<span class="math">\(c\)</span>の共起回数</li>
<li><span class="math">\(f(t,*)\)</span>: 単語<span class="math">\(t\)</span>の出現回数</li>
<li><span class="math">\(f(*,c)\)</span>: 文脈語<span class="math">\(c\)</span>の出現回数</li>
<li><span class="math">\(N\)</span>: 単語と文脈語のペアの総出現回数</li>
</ul>

      </div>
      
      <h3><a name="sec84"></a>84. 単語文脈行列の作成</h3>
      <div>
        <p>83の出力を利用し，単語文脈行列<span class="math">\(X\)</span>を作成せよ．ただし，行列<span class="math">\(X\)</span>の各要素<span class="math">\(X_{tc}\)</span>は次のように定義する．</p>
<ul>
<li><span class="math">\(f(t, c) \geq 10\)</span>ならば，<span class="math">\(X_{tc} = {\rm PPMI}(t, c) = \max\{\log \frac{N \times f(t,c)}{f(t,*) \times f(*,c)}, 0\}\)</span></li>
<li><span class="math">\(f(t, c) &lt; 10\)</span>ならば，<span class="math">\(X_{tc} = 0\)</span></li>
</ul>
<p>ここで，<span class="math">\({\rm PPMI}(t, c)\)</span>はPositive Pointwise Mutual Information（正の相互情報量）と呼ばれる統計量である．なお，行列<span class="math">\(X\)</span>の行数・列数は数百万オーダとなり，行列のすべての要素を主記憶上に載せることは無理なので注意すること．幸い，行列<span class="math">\(X\)</span>のほとんどの要素は<span class="math">\(0\)</span>になるので，非<span class="math">\(0\)</span>の要素だけを書き出せばよい．</p>

      </div>
      
      <h3><a name="sec85"></a>85. 主成分分析による次元圧縮</h3>
      <div>
        <p>84で得られた単語文脈行列に対して，主成分分析を適用し，単語の意味ベクトルを300次元に圧縮せよ．</p>

      </div>
      
      <h3><a name="sec86"></a>86. 単語ベクトルの表示</h3>
      <div>
        <p>85で得た単語の意味ベクトルを読み込み，&quot;United States&quot;のベクトルを表示せよ．ただし，&quot;United States&quot;は内部的には&quot;United_States&quot;と表現されていることに注意せよ．</p>

      </div>
      
      <h3><a name="sec87"></a>87. 単語の類似度</h3>
      <div>
        <p>85で得た単語の意味ベクトルを読み込み，&quot;United States&quot;と&quot;U.S.&quot;のコサイン類似度を計算せよ．ただし，&quot;U.S.&quot;は内部的に&quot;U.S&quot;と表現されていることに注意せよ．</p>

      </div>
      
      <h3><a name="sec88"></a>88. 類似度の高い単語10件</h3>
      <div>
        <p>85で得た単語の意味ベクトルを読み込み，&quot;England&quot;とコサイン類似度が高い10語と，その類似度を出力せよ．</p>

      </div>
      
      <h3><a name="sec89"></a>89. 加法構成性によるアナロジー</h3>
      <div>
        <p>85で得た単語の意味ベクトルを読み込み，vec(&quot;Spain&quot;) - vec(&quot;Madrid&quot;) + vec(&quot;Athens&quot;)を計算し，そのベクトルと類似度の高い10語とその類似度を出力せよ．</p>

      </div>
      
      
      <h2 class="page-header"><a name="ch10"></a>第10章: ベクトル空間法 (II)</h2>
      <div>
        <p>第10章では，前章に引き続き単語ベクトルの学習に取り組む．</p>

      </div>
      
      <h3><a name="sec90"></a>90. word2vecによる学習</h3>
      <div>
        <p>81で作成したコーパスに対して<a href="https://code.google.com/p/word2vec/">word2vec</a>を適用し，単語ベクトルを学習せよ．さらに，学習した単語ベクトルの形式を変換し，86-89のプログラムを動かせ．</p>

      </div>
      
      <h3><a name="sec91"></a>91. アナロジーデータの準備</h3>
      <div>
        <p><a href="https://word2vec.googlecode.com/svn/trunk/questions-words.txt">単語アナロジーの評価データ</a>をダウンロードせよ．このデータ中で&quot;: &quot;で始まる行はセクション名を表す．例えば，&quot;: capital-common-countries&quot;という行は，&quot;capital-common-countries&quot;というセクションの開始を表している．ダウンロードした評価データの中で，&quot;family&quot;というセクションに含まれる評価事例を抜き出してファイルに保存せよ．</p>

      </div>
      
      <h3><a name="sec92"></a>92. アナロジーデータへの適用</h3>
      <div>
        <p>91で作成した評価データの各事例に対して，vec(2列目の単語) - vec(1列目の単語) + vec(3列目の単語)を計算し，そのベクトルと類似度が最も高い単語と，その類似度を求めよ．求めた単語と類似度は，各事例の末尾に追記せよ．このプログラムを85で作成した単語ベクトル，90で作成した単語ベクトルに対して適用せよ．</p>

      </div>
      
      <h3><a name="sec93"></a>93. アナロジータスクの正解率の計算</h3>
      <div>
        <p>92で作ったデータを用い，各モデルのアナロジータスクの正解率を求めよ．</p>

      </div>
      
      <h3><a name="sec94"></a>94. WordSimilarity-353での類似度計算</h3>
      <div>
        <p><a href="http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/">The WordSimilarity-353 Test Collection</a>の評価データを入力とし，1列目と2列目の単語の類似度を計算し，各行の末尾に類似度の値を追加するプログラムを作成せよ．このプログラムを85で作成した単語ベクトル，90で作成した単語ベクトルに対して適用せよ．</p>

      </div>
      
      <h3><a name="sec95"></a>95. WordSimilarity-353での評価</h3>
      <div>
        <p>94で作ったデータを用い，各モデルが出力する類似度のランキングと，人間の類似度判定のランキングの間のスピアマン相関係数を計算せよ．</p>

      </div>
      
      <h3><a name="sec96"></a>96. 国名に関するベクトルの抽出</h3>
      <div>
        <p>word2vecの学習結果から，国名に関するベクトルのみを抜き出せ．</p>

      </div>
      
      <h3><a name="sec97"></a>97. k-meansクラスタリング</h3>
      <div>
        <p>96の単語ベクトルに対して，k-meansクラスタリングをクラスタ数<span class="math">\(k=5\)</span>として実行せよ．</p>

      </div>
      
      <h3><a name="sec98"></a>98. Ward法によるクラスタリング</h3>
      <div>
        <p>96の単語ベクトルに対して，Ward法による階層型クラスタリングを実行せよ．さらに，クラスタリング結果をデンドログラムとして可視化せよ．</p>

      </div>
      
      <h3><a name="sec99"></a>99. t-SNEによる可視化</h3>
      <div>
        <p>96の単語ベクトルに対して，ベクトル空間をt-SNEで可視化せよ．</p>

      </div>
      
      

      <h2 class="page-header"><a name="data"></a>100本ノックで用いるコーパス・データ</h2>
      <ul>
<li><a href="data/hightemp.txt">hightemp.txt</a>: <a href="http://www.jma.go.jp/">気象庁</a>が公開している「<a href="http://www.data.jma.go.jp/obd/stats/etrn/view/rankall.php?prec_no=&amp;block_no=&amp;year=&amp;month=&amp;day=&amp;view=">歴代全国ランキング&gt;観測史上の順位&gt;最高気温の高い方から</a>」を基に，手作業でタブ区切り形式に整形したものです．利用規約等は<a href="http://www.jma.go.jp/jma/kishou/info/coment.html">こちらのページ</a>を参照して下さい．</li>
<li><a href="data/jawiki-country.json.gz">jawiki-country.json.gz</a>: 2014年10月18日付けの<a href="http://dumps.wikimedia.org/jawiki/latest/jawiki-latest-pages-articles.xml.bz2">日本語のWikipedia記事のダンプ</a>の中から，国家に言及していると思われる記事を抽出し，JSON形式で格納したものです．このファイルは，<a href="http://creativecommons.org/licenses/by-sa/3.0/legalcode">クリエイティブ・コモンズ 表示-継承 3.0 非移植</a>のライセンスで配布されています．</li>
<li><a href="data/neko.txt">neko.txt</a>: <a href="http://www.aozora.gr.jp/">青空文庫</a>で公開されている夏目漱石の長編小説『吾輩は猫である』をテキストファイルに整形したものです．</li>
<li><a href="data/nlp.txt">nlp.txt</a>: 英語のWikipedia記事<a href="https://en.wikipedia.org/wiki/Natural_language_processing">&quot;Natural Language Processing&quot;</a>を１行１文形式にまとめたものです．このファイルは<a href="http://creativecommons.org/licenses/by-sa/3.0/legalcode">クリエイティブ・コモンズ 表示-継承 3.0 非移植</a>のライセンスで配布されています．</li>
<li><a href="data/artist.json.gz">artist.json.gz</a>: <a href="https://musicbrainz.org/">MusicBrainz</a>が公開している<a href="https://musicbrainz.org/doc/MusicBrainz_Database/Download">データベースのスナップショット</a>のうち，アーティストに関するものを抜き出し，JSON形式にまとめたものです．このファイルは<a href="http://creativecommons.org/licenses/by-nc-sa/3.0/">クリエイティブ・コモンズ 表示 - 非営利 - 継承 3.0 非移植</a>のライセンスで配布されています．</li>
<li><a href="data/enwiki-20150112-400-r10-105752.txt.bz2">enwiki-20150112-400-r10-105752.txt.bz2</a>: 2015年1月12日時点の<a href="http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2">Wikipedia記事データベースのダンプ（英語）</a>うち，約400語以上で構成される記事の中から，ランダムに1/10サンプリングした105,752記事のテキストをbzip2形式で圧縮したものです．このファイルは<a href="http://creativecommons.org/licenses/by-sa/3.0/legalcode">クリエイティブ・コモンズ 表示-継承 3.0 非移植</a>のライセンスで配布されています．</li>
</ul>


    </div> <!-- /container -->

    <footer class="bs-docs-footer" role="contentinfo">
      <div class="container">
        <p>Copyright (c) 2012-2015 <a href="http://www.chokkan.org/">Naoaki Okazaki</a>, <a href="http://www.cl.ecei.tohoku.ac.jp/">Inui-Okazaki Laboratory</a>.</p>
        <p>Updated at 2015-03-13 23:01:07 +0900</p>
        <p>Named by <a href="http://www.cl.ecei.tohoku.ac.jp/~jun-s/">Jun Sugiura</a>.</p>
        <p>Inspired by the exercise done in <a href="http://sslab.nuee.nagoya-u.ac.jp/">Sato Laboratory</a>.</p>
      </div> <!-- /container -->
    </footer>
      
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
    <script src="bootstrap/js/bootstrap.js"></script>
    <script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="bootstrap/js/ie10-viewport-bug-workaround.js"></script>

  </body>
</html>